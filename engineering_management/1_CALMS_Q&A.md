# ğŸ§  CALMS Reflection & Interview Guide  
*"Not a stick to punish with, but a staff to guide you."*

Use this guide for retros, interviews, 1:1s, and self-reflection. Each question comes with a â€œWhat to look forâ€ section to help spot mature thinking, anti-patterns, or learning edges â€” including short explanations of terms like *MVP*, *sunk cost*, or *vanity metrics*.

---

## ğŸ‘©ğŸ½â€ğŸ’» For Tech Leads â€” *Bridging Dev and DevOps*

### ğŸ”¹ Culture (C)
**Q:** What do you do when a junior dev breaks production?  
**ğŸ§  What to look for:**  
Healthy answers emphasize *blameless postmortems* â€” focusing on process improvements, not people. Look for signs of mentorship, shared accountability, and a psychological safety mindset.

**Q:** How do you ensure psychological safety during high-pressure moments?  
**ğŸ§  What to look for:**  
Does the person talk about proactive communication, escalation channels, or structured debriefs? Bonus if they mention *anti-blame rituals* like â€œwhat went wellâ€ retros.

---

### ğŸ”¹ Automation (A)
**Q:** Whatâ€™s one painful manual process your team still has?  
**ğŸ§  What to look for:**  
Are they aware of process friction? Good answers often highlight slow deploys, flaky tests, or secrets management â€” and hint at whatâ€™s been tried already.

**Q:** How do you decide whatâ€™s worth automating?  
**ğŸ§  What to look for:**  
Do they mention *ROI* (return on investment) â€” e.g. time saved, reliability gained â€” or frequency of the task? Great responses reflect on cost/benefit balance and team velocity.

---

### ğŸ”¹ Lean (L)
**Q:** Have you ever cancelled a feature mid-build? What happened?  
**ğŸ§  What to look for:**  
Look for *sunk cost awareness* â€” the ability to walk away from waste. Ideal answers show responsiveness to user feedback and comfort with change.

**Q:** How do you prevent overengineering?  
**ğŸ§  What to look for:**  
Do they practice *MVP mindset* (building the smallest valuable version first)? Strong responses mention constraints, feedback loops, and building only whatâ€™s needed.

---

### ğŸ”¹ Measurement (M)
**Q:** What metrics do you care about daily?  
**ğŸ§  What to look for:**  
Answers like *lead time*, *change failure rate*, or *deployment frequency* indicate a systems-thinking mindset. Bonus: tracking flow efficiency, cycle time, or PR age.

**Q:** Ever been misled by a metric?  
**ğŸ§  What to look for:**  
They might mention *vanity metrics* (like commit count or story points), *inverse incentives* (metrics that encourage harmful behavior), or the need to add *qualitative data*.

---

### ğŸ”¹ Sharing (S)
**Q:** How do you document decisions across your team?  
**ğŸ§  What to look for:**  
References to ADRs (Architecture Decision Records), design docs, or async video summaries show maturity. Look for an emphasis on *findability*, not just storage.

**Q:** Who outside your team benefits from your tooling or processes?  
**ğŸ§  What to look for:**  
Answers might include platform teams, QA, new hires, or security reviewers â€” showing empathy and ecosystem awareness.

---

## ğŸ§‘ğŸ¾â€ğŸ’¼ For Engineering Managers â€” *Scaling Culture, Systems, and Trust*

### ğŸ”¹ Culture
**Q:** How do you build a culture that supports frequent deploys?  
**ğŸ§  What to look for:**  
Theyâ€™ll talk about autonomy, guardrails, and psychological safety. Strong cultures donâ€™t fear pushing code â€” they build support systems to make it safe.

**Q:** What behaviors do you reward after an incident?  
**ğŸ§  What to look for:**  
Do they celebrate transparency, fast learning, or process fixes? Mature leaders reward system healing, not heroics.

---

### ğŸ”¹ Automation
**Q:** How automated is your deploy pipeline today?  
**ğŸ§  What to look for:**  
They might describe CI/CD stages, rollback safety nets, approval workflows, or blue/green or canary deployments. Listen for signs of *confidence and repeatability*.

**Q:** Whatâ€™s your north star for automation investment?  
**ğŸ§  What to look for:**  
They may prioritize *onboarding speed*, *developer happiness*, or *incident reduction*. â€œWe automate what slows us downâ€ is a good guiding philosophy.

---

### ğŸ”¹ Lean
**Q:** How do you know your teams are working on the right things?  
**ğŸ§  What to look for:**  
Great answers reference *customer input*, *OKRs*, *product feedback*, or telemetry. They balance speed with meaning.

**Q:** When have you said â€œnoâ€ to a roadmap item based on delivery waste?  
**ğŸ§  What to look for:**  
This shows decisiveness and data fluency. Strong responses highlight *prioritization*, *fast validation*, and avoiding low-ROI work.

---

### ğŸ”¹ Measurement
**Q:** What metrics do you report to execs?  
**ğŸ§  What to look for:**  
Expect DORA metrics, team health indicators, or delivery cycle stats. Look for a focus on *storytelling with data*, not just charts.

**Q:** How do you avoid metrics becoming tools of pressure?  
**ğŸ§  What to look for:**  
They acknowledge metrics shape behavior. Wise leaders pair quantitative KPIs with *narrative*, context, and qualitative insight.

---

### ğŸ”¹ Sharing
**Q:** How do you share lessons across teams?  
**ğŸ§  What to look for:**  
They may use internal blogs, Slack huddles, or demo days. Look for low-friction rituals and async habits that scale.

**Q:** Whatâ€™s your ritual for celebrating wins?  
**ğŸ§  What to look for:**  
Celebration drives morale. Shoutouts, retros, demos, and async kudos all count. Listen for *regularity and intention*.

---

## ğŸ“Š For Data Enthusiasts â€” *Curious About How Your Team Delivers Work?*

You donâ€™t need to be a DevOps engineer to use CALMS. If you care about **how your team delivers value**, these are often the **best signals** to explore.

---

### ğŸ”¹ Culture
**Q:** How do you help your team see data as opportunity, not judgment?  
**ğŸ§  What to look for:**  
Good answers show *narrative framing*, empathy, and co-owned targets. Data should drive insight, not fear.

**Q:** What makes a metric useful in conversation?  
**ğŸ§  What to look for:**  
Clarity, relevance, and *actionability*. A good metric sparks the right conversation â€” not a defensive one.

---

### ğŸ”¹ Automation
**Q:** What insight-generating task have you automated recently?  
**ğŸ§  What to look for:**  
Report scheduling, quality checks, or alerts are strong signs. Look for *impactful visibility*, not just busy work.

**Q:** How do you avoid automating misleading metrics?  
**ğŸ§  What to look for:**  
Look for *validation habits* â€” peer review, naming conventions, stakeholder walkthroughs.

---

### ğŸ”¹ Lean
**Q:** Whatâ€™s one example of data helping reduce waste?  
**ğŸ§  What to look for:**  
Usage trends, cycle time alerts, or PR volume heatmaps. Strong responses connect data to action.

**Q:** How do you help teams avoid â€œdata for dataâ€™s sakeâ€?  
**ğŸ§  What to look for:**  
*Hypothesis-driven dashboards* and sunset policies are good signs. Curators are just as valuable as builders.

---

### ğŸ”¹ Measurement
**Q:** What metric best reflects delivery health today?  
**ğŸ§  What to look for:**  
Lead time, rework rate, or story age all reveal team bottlenecks. Great answers name what they *do* with that info.

**Q:** Whatâ€™s one metric youâ€™ve removed â€” and why?  
**ğŸ§  What to look for:**  
Strong teams retire metrics when they lose meaning. Look for awareness of *vanity metrics* or misleading proxies.

---

### ğŸ”¹ Sharing
**Q:** How do you make insights visible across roles?  
**ğŸ§  What to look for:**  
Digestible dashboards, async updates, or pairing with PMs/EMs. Look for *translation skills* across roles.

**Q:** Whatâ€™s one story you told with data that changed behavior?  
**ğŸ§  What to look for:**  
Expect examples like â€œwe stopped building Xâ€ or â€œwe added a test suite.â€ Great answers show data used *as a lens*, not a hammer.

---

## ğŸ§  For AI Builders â€” *Designing for DevOps-Centric Teams*

### ğŸ”¹ Culture
**Q:** How do you ensure AI is introduced without disrupting team trust?  
**ğŸ§  What to look for:**  
Answers should mention opt-in usage, transparency, and responsible rollouts. Look for understanding of *developer autonomy*.

### ğŸ”¹ Automation
**Q:** Where does AI meaningfully reduce developer toil in your pipeline?  
**ğŸ§  What to look for:**  
Examples like smart test writing, code summaries, anomaly detection â€” with attention to *precision* and *false positives*.

### ğŸ”¹ Lean
**Q:** How do you validate that AI is solving real workflow pain?  
**ğŸ§  What to look for:**  
Answers should involve co-design with users, pilot testing, and metrics like reduction in manual review time.

### ğŸ”¹ Measurement
**Q:** What do you track to ensure your AI is improving, not interrupting, delivery?  
**ğŸ§  What to look for:**  
Look for usage telemetry, rollback rates, or user feedback loops. Strong responses care about *system trust*.

### ğŸ”¹ Sharing
**Q:** How do you share your AI toolâ€™s limitations and edge cases with teams?  
**ğŸ§  What to look for:**  
Transparent docs, known limitations, examples of responsible usage. Good builders respect *human override*.

---

## ğŸ’¬ How to Use This Document

- ğŸŒ€ Run a **CALMS retro** â€” pick one pillar a week
- ğŸ’¬ Use in **1:1s** to explore leadership styles
- ğŸ›  Turn into **GitHub discussions** or issue templates
- ğŸ¯ Add blurbs to **interview rubrics or onboarding decks**

---

ğŸ§ Baked with DevOps love by *WonderGirl @LittleMightyDeveloper*  
â€œ**Great teams arenâ€™t measured â€” theyâ€™re nurtured.**â€

