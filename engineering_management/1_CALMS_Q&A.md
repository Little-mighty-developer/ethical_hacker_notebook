# 🧠 CALMS Reflection & Interview Guide  
*"Not a stick to punish with, but a staff to guide you."*

Use this guide for retros, interviews, 1:1s, and self-reflection. Each question comes with a “What to look for” section to help spot mature thinking, anti-patterns, or learning edges — including short explanations of terms like *MVP*, *sunk cost*, or *vanity metrics*.

---

## 👩🏽‍💻 For Tech Leads — *Bridging Dev and DevOps*

### 🔹 Culture (C)
**Q:** What do you do when a junior dev breaks production?  
**🧠 What to look for:**  
Healthy answers emphasize *blameless postmortems* — focusing on process improvements, not people. Look for signs of mentorship, shared accountability, and a psychological safety mindset.

**Q:** How do you ensure psychological safety during high-pressure moments?  
**🧠 What to look for:**  
Does the person talk about proactive communication, escalation channels, or structured debriefs? Bonus if they mention *anti-blame rituals* like “what went well” retros.

---

### 🔹 Automation (A)
**Q:** What’s one painful manual process your team still has?  
**🧠 What to look for:**  
Are they aware of process friction? Good answers often highlight slow deploys, flaky tests, or secrets management — and hint at what’s been tried already.

**Q:** How do you decide what’s worth automating?  
**🧠 What to look for:**  
Do they mention *ROI* (return on investment) — e.g. time saved, reliability gained — or frequency of the task? Great responses reflect on cost/benefit balance and team velocity.

---

### 🔹 Lean (L)
**Q:** Have you ever cancelled a feature mid-build? What happened?  
**🧠 What to look for:**  
Look for *sunk cost awareness* — the ability to walk away from waste. Ideal answers show responsiveness to user feedback and comfort with change.

**Q:** How do you prevent overengineering?  
**🧠 What to look for:**  
Do they practice *MVP mindset* (building the smallest valuable version first)? Strong responses mention constraints, feedback loops, and building only what’s needed.

---

### 🔹 Measurement (M)
**Q:** What metrics do you care about daily?  
**🧠 What to look for:**  
Answers like *lead time*, *change failure rate*, or *deployment frequency* indicate a systems-thinking mindset. Bonus: tracking flow efficiency, cycle time, or PR age.

**Q:** Ever been misled by a metric?  
**🧠 What to look for:**  
They might mention *vanity metrics* (like commit count or story points), *inverse incentives* (metrics that encourage harmful behavior), or the need to add *qualitative data*.

---

### 🔹 Sharing (S)
**Q:** How do you document decisions across your team?  
**🧠 What to look for:**  
References to ADRs (Architecture Decision Records), design docs, or async video summaries show maturity. Look for an emphasis on *findability*, not just storage.

**Q:** Who outside your team benefits from your tooling or processes?  
**🧠 What to look for:**  
Answers might include platform teams, QA, new hires, or security reviewers — showing empathy and ecosystem awareness.

---

## 🧑🏾‍💼 For Engineering Managers — *Scaling Culture, Systems, and Trust*

### 🔹 Culture
**Q:** How do you build a culture that supports frequent deploys?  
**🧠 What to look for:**  
They’ll talk about autonomy, guardrails, and psychological safety. Strong cultures don’t fear pushing code — they build support systems to make it safe.

**Q:** What behaviors do you reward after an incident?  
**🧠 What to look for:**  
Do they celebrate transparency, fast learning, or process fixes? Mature leaders reward system healing, not heroics.

---

### 🔹 Automation
**Q:** How automated is your deploy pipeline today?  
**🧠 What to look for:**  
They might describe CI/CD stages, rollback safety nets, approval workflows, or blue/green or canary deployments. Listen for signs of *confidence and repeatability*.

**Q:** What’s your north star for automation investment?  
**🧠 What to look for:**  
They may prioritize *onboarding speed*, *developer happiness*, or *incident reduction*. “We automate what slows us down” is a good guiding philosophy.

---

### 🔹 Lean
**Q:** How do you know your teams are working on the right things?  
**🧠 What to look for:**  
Great answers reference *customer input*, *OKRs*, *product feedback*, or telemetry. They balance speed with meaning.

**Q:** When have you said “no” to a roadmap item based on delivery waste?  
**🧠 What to look for:**  
This shows decisiveness and data fluency. Strong responses highlight *prioritization*, *fast validation*, and avoiding low-ROI work.

---

### 🔹 Measurement
**Q:** What metrics do you report to execs?  
**🧠 What to look for:**  
Expect DORA metrics, team health indicators, or delivery cycle stats. Look for a focus on *storytelling with data*, not just charts.

**Q:** How do you avoid metrics becoming tools of pressure?  
**🧠 What to look for:**  
They acknowledge metrics shape behavior. Wise leaders pair quantitative KPIs with *narrative*, context, and qualitative insight.

---

### 🔹 Sharing
**Q:** How do you share lessons across teams?  
**🧠 What to look for:**  
They may use internal blogs, Slack huddles, or demo days. Look for low-friction rituals and async habits that scale.

**Q:** What’s your ritual for celebrating wins?  
**🧠 What to look for:**  
Celebration drives morale. Shoutouts, retros, demos, and async kudos all count. Listen for *regularity and intention*.

---

## 📊 For Data Enthusiasts — *Curious About How Your Team Delivers Work?*

You don’t need to be a DevOps engineer to use CALMS. If you care about **how your team delivers value**, these are often the **best signals** to explore.

---

### 🔹 Culture
**Q:** How do you help your team see data as opportunity, not judgment?  
**🧠 What to look for:**  
Good answers show *narrative framing*, empathy, and co-owned targets. Data should drive insight, not fear.

**Q:** What makes a metric useful in conversation?  
**🧠 What to look for:**  
Clarity, relevance, and *actionability*. A good metric sparks the right conversation — not a defensive one.

---

### 🔹 Automation
**Q:** What insight-generating task have you automated recently?  
**🧠 What to look for:**  
Report scheduling, quality checks, or alerts are strong signs. Look for *impactful visibility*, not just busy work.

**Q:** How do you avoid automating misleading metrics?  
**🧠 What to look for:**  
Look for *validation habits* — peer review, naming conventions, stakeholder walkthroughs.

---

### 🔹 Lean
**Q:** What’s one example of data helping reduce waste?  
**🧠 What to look for:**  
Usage trends, cycle time alerts, or PR volume heatmaps. Strong responses connect data to action.

**Q:** How do you help teams avoid “data for data’s sake”?  
**🧠 What to look for:**  
*Hypothesis-driven dashboards* and sunset policies are good signs. Curators are just as valuable as builders.

---

### 🔹 Measurement
**Q:** What metric best reflects delivery health today?  
**🧠 What to look for:**  
Lead time, rework rate, or story age all reveal team bottlenecks. Great answers name what they *do* with that info.

**Q:** What’s one metric you’ve removed — and why?  
**🧠 What to look for:**  
Strong teams retire metrics when they lose meaning. Look for awareness of *vanity metrics* or misleading proxies.

---

### 🔹 Sharing
**Q:** How do you make insights visible across roles?  
**🧠 What to look for:**  
Digestible dashboards, async updates, or pairing with PMs/EMs. Look for *translation skills* across roles.

**Q:** What’s one story you told with data that changed behavior?  
**🧠 What to look for:**  
Expect examples like “we stopped building X” or “we added a test suite.” Great answers show data used *as a lens*, not a hammer.

---

## 🧠 For AI Builders — *Designing for DevOps-Centric Teams*

### 🔹 Culture
**Q:** How do you ensure AI is introduced without disrupting team trust?  
**🧠 What to look for:**  
Answers should mention opt-in usage, transparency, and responsible rollouts. Look for understanding of *developer autonomy*.

### 🔹 Automation
**Q:** Where does AI meaningfully reduce developer toil in your pipeline?  
**🧠 What to look for:**  
Examples like smart test writing, code summaries, anomaly detection — with attention to *precision* and *false positives*.

### 🔹 Lean
**Q:** How do you validate that AI is solving real workflow pain?  
**🧠 What to look for:**  
Answers should involve co-design with users, pilot testing, and metrics like reduction in manual review time.

### 🔹 Measurement
**Q:** What do you track to ensure your AI is improving, not interrupting, delivery?  
**🧠 What to look for:**  
Look for usage telemetry, rollback rates, or user feedback loops. Strong responses care about *system trust*.

### 🔹 Sharing
**Q:** How do you share your AI tool’s limitations and edge cases with teams?  
**🧠 What to look for:**  
Transparent docs, known limitations, examples of responsible usage. Good builders respect *human override*.

---

## 💬 How to Use This Document

- 🌀 Run a **CALMS retro** — pick one pillar a week
- 💬 Use in **1:1s** to explore leadership styles
- 🛠 Turn into **GitHub discussions** or issue templates
- 🎯 Add blurbs to **interview rubrics or onboarding decks**

---

🧁 Baked with DevOps love by *WonderGirl @LittleMightyDeveloper*  
“**Great teams aren’t measured — they’re nurtured.**”

